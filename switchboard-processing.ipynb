{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Switchboard\n",
    "\n",
    "This notebook contains snippets of code that details how I processed the raw switchboard-1 corpus for fine-tuning Llama & generating ngram models.\n",
    "\n",
    "Vincent Danys\n",
    "2024-07-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from scripts.settings import Config\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class Transcript:\n",
    "  def __init__(self, id, lines, topic=\"UNAVAILABLE\"):\n",
    "    self.id = id\n",
    "    self.lines = lines\n",
    "    self.topic = topic\n",
    "\n",
    "class Line:\n",
    "  def __init__(self, speaker, id, start, end, text):\n",
    "    self.speaker = speaker\n",
    "    self.id = id\n",
    "    self.start = start\n",
    "    self.end = end\n",
    "    self.text = text\n",
    "\n",
    "# meta data\n",
    "SW_DIR = Config.SW_SOURCE\n",
    "\n",
    "# output directories\n",
    "OUTPUT_WITH_SPEAKERS = Config.SW_WITH_SPEAKER_DIR\n",
    "OUTPUT_NO_SPEAKERS = Config.SW_NO_SPEAKER_DIR\n",
    "OUTPUT_INSTRUCT = Config.SW_INSTRUCT_NO_SPEAKERS\n",
    "\n",
    "try: os.mkdir(OUTPUT_WITH_SPEAKERS)\n",
    "except: pass\n",
    "try: os.mkdir(OUTPUT_NO_SPEAKERS)\n",
    "except: pass\n",
    "try: os.mkdir(OUTPUT_INSTRUCT)\n",
    "except: pass\n",
    "\n",
    "\n",
    "# speaker prefixes. To generate data with no speaker information, see below.\n",
    "Speaker1 = \"Speaker 1: \"\n",
    "Speaker2 = \"Speaker 2: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_swb_file_list\n",
    "switchboard-1 should contain `20/` `21/` ... `49/` under which sit `20/2001` and etc. This function returns a list of all switchboard transcript files, sorted by conversation.\n",
    "\n",
    "**Note**: this function relies on the existence of `AAREADME.text` in the switchboard directory, as that is where all the file directories are extracted from.\n",
    "\n",
    "E.g.,:\n",
    "* `return[0] = \"/21/2101/sw2101A-ms98-a-trans.text\"`\n",
    "* `return[1] = \"/21/2101/sw2101A-ms98-a-word.text\"`\n",
    "* `return[5] = \"/21/2102/sw2102A-ms98-a-trans.text\"`\n",
    "* etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swb_file_list():\n",
    "  swb_files = []\n",
    "  \n",
    "  with open(SW_DIR + \"AAREADME.text\", \"r\") as readme_file:\n",
    "    readme_lines = readme_file.read().split('\\n')\n",
    "\n",
    "  for i in range(17, len(readme_lines), 5):\n",
    "\n",
    "    subdir = readme_lines[i]\n",
    "    files = readme_lines[i+1:i+5]\n",
    "\n",
    "    for file in files:\n",
    "      swb_files.append(SW_DIR + subdir[:2] + \"/\" + subdir + \"/\" + file.strip())\n",
    "\n",
    "  return swb_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(get_swb_file_list()) / 4 == 2438 # full, unchanged switchboard-1 dataset should pass this test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean_transcript_swb\n",
    "Takes the transcript, in special line objects, and cleans the text.\n",
    "* Removes [laughter], [noise], and [vocalized-noise]\n",
    "* Removes any other [laughter-BLANK] combination\n",
    "* Converts word fragments into words. E.g., \"I gue[ss]\" -> \"I guess\"\n",
    "* Removes all text between `<b_aside>` and `<e_aside>`\n",
    "* Curly bracket words are treated as normal words. \"{federaldes}\" -> \"federaldes\"\n",
    "* Capitalizes the start of each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted code originally written by Mai (s2324822)\n",
    "def clean_transcript_swb(transcript_lines: list[Line]):\n",
    "\n",
    "  clean_lines = []\n",
    "\n",
    "  for line in transcript_lines:\n",
    "\n",
    "    text = line.text\n",
    "\n",
    "    # remove speech events\n",
    "    clean = re.sub(r'\\[laughter\\]|\\[noise\\]|\\[vocalized-noise\\]', '', text)\n",
    "\n",
    "    # check if further cleaning required\n",
    "    if '[' in text:\n",
    "      clean_tokens = []\n",
    "      tokens = clean.split(' ')\n",
    "\n",
    "      for token in tokens:\n",
    "\n",
    "        # special laughter case\n",
    "        if '[laughter-' in token:\n",
    "          token = re.sub(r'\\[laughter-', '', token)\n",
    "\n",
    "        # partial completions\n",
    "        if '-[' in token or ']-' in token:\n",
    "          token = re.sub(r'\\-\\[|\\]\\-', '', token)\n",
    "          \n",
    "        token = re.sub(r'\\[|\\]', '', token)\n",
    "        clean_tokens.append(token)\n",
    "\n",
    "      clean = ' '.join(clean_tokens)\n",
    "\n",
    "    # <aside> text\n",
    "    if \"<b_aside>\" in clean:\n",
    "      clean = re.sub(r'<b_aside>.*?<e_aside>', '', clean)\n",
    "\n",
    "    # special pronounciation (eg. 'because_1')\n",
    "    found = [i for i in range(len(clean)) if clean.startswith('_', i)]\n",
    "    if len(found) > 0:\n",
    "      f_indxs = [-2] + found + [len(clean)]\n",
    "      clean = ''.join([clean[f_indxs[i-1]+2:f_indxs[i]] for i in range(1, len(f_indxs))])\n",
    "\n",
    "    clean = re.sub(' +', ' ', clean.strip())\n",
    "\n",
    "    # curly bracket text turned into normal words\n",
    "    clean = re.sub('\\{|\\}', '', clean)\n",
    "\n",
    "    # capitalize start of line for the ease of Llama 3\n",
    "    clean = clean.capitalize()\n",
    "    \n",
    "    if clean != \"\": clean_lines.append(Line(line.speaker, line.id, line.start, line.end, clean))\n",
    "\n",
    "  return clean_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_speaker_lines\n",
    "Switchboard transcribes each conversation into 2 separate files: speaker A and speaker B. The function `get_speaker_lines` converts one of these files into a list of `Line` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker_lines(speaker, dir):\n",
    "\n",
    "  with open(dir, \"r\") as file:\n",
    "    lines = file.read().split('\\n')\n",
    "\n",
    "  transcript_lines = []\n",
    "\n",
    "  for line in lines:\n",
    "\n",
    "    if line == \"\": continue\n",
    "\n",
    "    # remove extra spaces\n",
    "    if \" \"*6 in line:\n",
    "      line = line.replace(\" \"*6, \" \")\n",
    "      line = line.replace(\" \"*5, \" \")\n",
    "    line = line.replace(\"\t\", \" \")\n",
    "\n",
    "    # convert into line object\n",
    "    line_parts = line.split(\" \", 3)\n",
    "    if line_parts[3] == \"[silence]\": continue\n",
    "\n",
    "    transcript_lines.append(Line(\n",
    "      speaker,\n",
    "      line_parts[0],\n",
    "      float(line_parts[1]),\n",
    "      float(line_parts[2]),\n",
    "      line_parts[3]\n",
    "    ))\n",
    "\n",
    "  return transcript_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conversation_to_file\n",
    "Takes a list of `Line` objects and writes it in text format into a file.\n",
    "Optional settings:\n",
    "* `speakers: bool` - include speaker prefixes for each line\n",
    "* `text: bool` - include utterance text for each line\n",
    "* `start: bool` - include utterance start time\n",
    "* `end: bool` - include utterance end time\n",
    "* `topic: bool` - include the conversation topic as a header for the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_to_str(transcript, speakers=True, text=True, start=False, end=False, topic=False):\n",
    "\n",
    "  str_lines = []\n",
    "\n",
    "  if topic:\n",
    "    str_lines.append(f\"Topic: {transcript.topic}\")\n",
    "\n",
    "  for line in transcript.lines:\n",
    "\n",
    "    line_str = \"\"\n",
    "\n",
    "    if start:\n",
    "      line_str += f\"[{line.start:.2f} \"\n",
    "\n",
    "    if end:\n",
    "      line_str += f\"[{line.end:.2f}] \"\n",
    "\n",
    "    if speakers:\n",
    "      line_str += line.speaker\n",
    "\n",
    "    if text:\n",
    "      line_str += line.text\n",
    "    \n",
    "    str_lines.append(line_str)\n",
    "  \n",
    "  return str_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### swb_format_chat_template\n",
    "\n",
    "Takes a list of strings of a conversation and formats it with a prompt in a style of a conversation between a user and AI-assistant. This prepares the data for training an instruction model, like Llama-3-8B-Instruct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = [\"Write a phone conversation between two people. The first few lines of the conversation are:\\n\",\n",
    "          \"\\n\\nWrite 100 more lines.\"]\n",
    "\n",
    "MESSAGES = [\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a chatbot that writes text based on the user's input.\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\n",
    "  }\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME_OR_DIR)\n",
    "\n",
    "def swb_format_chat_template(transcript_lines): \n",
    "\n",
    "  global tokenizer\n",
    "  \n",
    "  first_four = transcript_lines[:4]\n",
    "  continuation = transcript_lines[4:]\n",
    "\n",
    "  MESSAGES[1][\"content\"] = PROMPT[0] + '\\n'.join(first_four) + PROMPT[1]\n",
    "  content = tokenizer.apply_chat_template(MESSAGES, tokenize=False, add_generation_prompt=True)\n",
    "  content += \"Here is the continuation of the conversation:\\n\\n\" + '\\n'.join(continuation) + \"<|eot_id|>\"\n",
    "\n",
    "  return content.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_lines_to_file(filename, str_lines):\n",
    "  with open(filename, \"w\") as file:\n",
    "    for line in str_lines:\n",
    "      file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main\n",
    "Reads the filenames from `file-list.text`. Reads transcript files, cleans up the transcripts, and writes them to a directory with speakers and with no speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  \n",
    "  global get_swb_file_list, str_lines_to_file, swb_format_chat_template\n",
    "\n",
    "  file_list = get_swb_file_list()\n",
    "\n",
    "  for i in range(0, len(file_list), 4):\n",
    "\n",
    "    transcript_id = file_list[i].split(\"/\")[-2]\n",
    "    speaker_a_dir = file_list[i]\n",
    "    # file_list[i+1] is the transcript word-by-word\n",
    "    speaker_b_dir = file_list[i+2]\n",
    "    # file_list[i+3] is the transcript word-by-word\n",
    "\n",
    "    # get conversation transcript\n",
    "    speaker_a_lines = get_speaker_lines(Speaker1, speaker_a_dir)\n",
    "    speaker_b_lines = get_speaker_lines(Speaker2, speaker_b_dir)\n",
    "    transcript = Transcript(transcript_id, speaker_a_lines + speaker_b_lines)\n",
    "    transcript.lines.sort(key=lambda x: x.start)\n",
    "\n",
    "    # clean transcript\n",
    "    transcript.lines = clean_transcript_swb(transcript.lines)\n",
    "\n",
    "    # write transcript with speakers\n",
    "    str_lines_to_file(OUTPUT_WITH_SPEAKERS + transcript_id + \".text\", conversation_to_str(transcript, speakers=True)) \n",
    "\n",
    "    # write transcript without speakers\n",
    "    str_lines_to_file(OUTPUT_NO_SPEAKERS + transcript_id + \".text\", conversation_to_str(transcript, speakers=False))\n",
    "\n",
    "    # write transcript with instruction\n",
    "    str_lines_to_file(OUTPUT_INSTRUCT + transcript_id + \".text\", swb_format_chat_template(conversation_to_str(transcript, speakers=False)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
