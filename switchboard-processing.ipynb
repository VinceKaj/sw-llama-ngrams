{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Switchboard\n",
    "\n",
    "This notebook contains snippets of code that details how I processed the raw switchboard-1 corpus for fine-tuning Llama & generating ngram models.\n",
    "\n",
    "Vincent Danys\n",
    "2024-07-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "dir = os.path.dirname(os.path.realpath(__name__))\n",
    "\n",
    "class Transcript:\n",
    "  def __init__(self, id, lines, topic):\n",
    "    self.id = id\n",
    "    self.lines = lines\n",
    "    self.topic = topic\n",
    "\n",
    "class Line:\n",
    "  def __init__(self, speaker, id, start, end, text):\n",
    "    self.speaker = speaker\n",
    "    self.id = id\n",
    "    self.start = start\n",
    "    self.end = end\n",
    "    self.text = text\n",
    "\n",
    "# meta data\n",
    "SW_DIR = dir + \"/switchboard-1/\"\n",
    "FILE_LIST_DIR = dir + \"/file-list.text\"\n",
    "TOPICS_DIR = dir + \"/topics.text\"\n",
    "\n",
    "# output directories\n",
    "OUTPUT_WITH_SPEAKERS = dir + \"/output/with-speakers/\"\n",
    "OUTPUT_NO_SPEAKERS = dir + \"/output/no-speakers/\"\n",
    "\n",
    "os.mkdir(dir + \"/output\")\n",
    "os.mkdir(OUTPUT_WITH_SPEAKERS)\n",
    "os.mkdir(OUTPUT_NO_SPEAKERS)\n",
    "\n",
    "# speaker prefixes. To generate data with no speaker information, see below.\n",
    "Speaker1 = \"Speaker 1: \"\n",
    "Speaker2 = \"Speaker 2: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean_transcript_swb\n",
    "Takes the transcript, in special line objects, and cleans the text.\n",
    "* Removes [laughter], [noise], and [vocalized-noise]\n",
    "* Removes any other [laughter-BLANK] combination\n",
    "* Converts word fragments into words. E.g., \"I gue[ss]\" -> \"I guess\"\n",
    "* Removes all text between `<b_aside>` and `<e_aside>`\n",
    "* Curly bracket words are treated as normal words. \"{federaldes}\" -> \"federaldes\"\n",
    "* Capitalizes the start of each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted code originally written by Mai (s2324822)\n",
    "def clean_transcript_swb(transcript_lines: list[Line]):\n",
    "\n",
    "  clean_lines = []\n",
    "\n",
    "  for line in transcript_lines:\n",
    "\n",
    "    text = line.text\n",
    "\n",
    "    # remove speech events\n",
    "    clean = re.sub(r'\\[laughter\\]|\\[noise\\]|\\[vocalized-noise\\]', '', text)\n",
    "\n",
    "    # check if further cleaning required\n",
    "    if '[' in text:\n",
    "      clean_tokens = []\n",
    "      tokens = clean.split(' ')\n",
    "\n",
    "      for token in tokens:\n",
    "\n",
    "        # special laughter case\n",
    "        if '[laughter-' in token:\n",
    "          token = re.sub(r'\\[laughter-', '', token)\n",
    "\n",
    "        # partial completions\n",
    "        if '-[' in token or ']-' in token:\n",
    "          token = re.sub(r'\\-\\[|\\]\\-', '', token)\n",
    "          \n",
    "        token = re.sub(r'\\[|\\]', '', token)\n",
    "        clean_tokens.append(token)\n",
    "\n",
    "      clean = ' '.join(clean_tokens)\n",
    "\n",
    "    # <aside> text\n",
    "    if \"<b_aside>\" in clean:\n",
    "      clean = re.sub(r'<b_aside>.*?<e_aside>', '', clean)\n",
    "\n",
    "    # special pronounciation (eg. 'because_1')\n",
    "    found = [i for i in range(len(clean)) if clean.startswith('_', i)]\n",
    "    if len(found) > 0:\n",
    "      f_indxs = [-2] + found + [len(clean)]\n",
    "      clean = ''.join([clean[f_indxs[i-1]+2:f_indxs[i]] for i in range(1, len(f_indxs))])\n",
    "\n",
    "    clean = re.sub(' +', ' ', clean.strip())\n",
    "\n",
    "    # curly bracket text turned into normal words\n",
    "    clean = re.sub('\\{|\\}', '', clean)\n",
    "\n",
    "    # capitalize start of line for the ease of Llama 3\n",
    "    clean = clean.capitalize()\n",
    "    \n",
    "    if clean != \"\": clean_lines.append(Line(line.speaker, line.id, line.start, line.end, clean))\n",
    "\n",
    "  return clean_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_speaker_lines\n",
    "Switchboard transcribes each conversation into 2 separate files: speaker A and speaker B. The function `get_speaker_lines` converts one of these files into a list of `Line` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker_lines(speaker, dir):\n",
    "\n",
    "  with open(dir, \"r\") as file:\n",
    "    lines = file.read().split('\\n')\n",
    "\n",
    "  transcript_lines = []\n",
    "\n",
    "  for line in lines:\n",
    "\n",
    "    if line == \"\": continue\n",
    "\n",
    "    # remove extra spaces\n",
    "    if \" \"*6 in line:\n",
    "      line = line.replace(\" \"*6, \" \")\n",
    "      line = line.replace(\" \"*5, \" \")\n",
    "    line = line.replace(\"\t\", \" \")\n",
    "\n",
    "    # convert into line object\n",
    "    line_parts = line.split(\" \", 3)\n",
    "    if line_parts[3] == \"[silence]\": continue\n",
    "\n",
    "    transcript_lines.append(Line(\n",
    "      speaker,\n",
    "      line_parts[0],\n",
    "      float(line_parts[1]),\n",
    "      float(line_parts[2]),\n",
    "      line_parts[3]\n",
    "    ))\n",
    "\n",
    "  return transcript_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conversation_to_file\n",
    "Takes a list of `Line` objects and writes it in text format into a file.\n",
    "Optional settings:\n",
    "* `speakers: bool` - include speaker prefixes for each line\n",
    "* `text: bool` - include utterance text for each line\n",
    "* `start: bool` - include utterance start time\n",
    "* `end: bool` - include utterance end time\n",
    "* `topic: bool` - include the conversation topic as a header for the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_to_file(transcript, file_dir, speakers=True, text=True, start=False, end=False, topic=False):\n",
    "\n",
    "  with (open(file_dir, \"w\")) as file:\n",
    "\n",
    "    if topic:\n",
    "      file.write(f\"Topic: {transcript.topic}\\n\")\n",
    "\n",
    "    for line in transcript.lines:\n",
    "\n",
    "      if start:\n",
    "        file.write(f\"[{line.start:.2f} \")\n",
    "\n",
    "      if end:\n",
    "        file.write(f\"[{line.end:.2f}] \")\n",
    "\n",
    "      if speakers:\n",
    "        file.write(line.speaker)\n",
    "\n",
    "      if text:\n",
    "        file.write(line.text)\n",
    "      \n",
    "      file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main\n",
    "Reads the filenames from `file-list.text`. Reads transcript files, cleans up the transcripts, and writes them to a directory with speakers and with no speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  \n",
    "  with open(FILE_LIST_DIR, \"r\") as file:\n",
    "    file_list = file.read().split('\\n')\n",
    "\n",
    "  with open(TOPICS_DIR, \"r\") as file:\n",
    "    topics_lines = file.read().split('\\n')\n",
    "    topics = {x.split(\"\\t\")[0]: x.split(\"\\t\")[3] for x in topics_lines}\n",
    "\n",
    "  for i in range(0, len(file_list), 4):\n",
    "\n",
    "    transcript_id = file_list[i].split(\"/\")[1]\n",
    "    speaker_a_dir = SW_DIR + file_list[i]\n",
    "    # file_list[i+1] is the transcript word-by-word\n",
    "    speaker_b_dir = SW_DIR + file_list[i+2]\n",
    "    # file_list[i+3] is the transcript word-by-word\n",
    "\n",
    "    # get conversation transcript\n",
    "    speaker_a_lines = get_speaker_lines(Speaker1, speaker_a_dir)\n",
    "    speaker_b_lines = get_speaker_lines(Speaker2, speaker_b_dir)\n",
    "    transcript = Transcript(transcript_id, speaker_a_lines + speaker_b_lines, topics[transcript_id])\n",
    "    transcript.lines.sort(key=lambda x: x.start)\n",
    "\n",
    "    # clean transcript\n",
    "    transcript.lines = clean_transcript_swb(transcript.lines)\n",
    "\n",
    "    # write transcript with speakers\n",
    "    conversation_to_file(transcript, OUTPUT_WITH_SPEAKERS + transcript_id + \".text\", speakers=True)\n",
    "\n",
    "    # write transcript without speakers\n",
    "    conversation_to_file(transcript, OUTPUT_NO_SPEAKERS + transcript_id + \".text\", speakers=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
