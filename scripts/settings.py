from transformers import BitsAndBytesConfig
import torch
import os

dir = os.path.dirname(os.path.abspath(__file__))

class Config:

  # GENERAL
  SEED = 42 # for data shuffling. NOTE: determines how the data is split for tuning llama, prompting for generation, and testing

  # DATA
  SW_SOURCE = f"{dir}/../data/switchboard-1/" # switchboard source directory
  SW_NO_SPEAKER_DIR = f"{dir}/../data/sw-no-speakers/"
  SW_WITH_SPEAKER_DIR = f"{dir}/../data/sw-with-speakers/"
  SW_INSTRUCT_NO_SPEAKERS = f"{dir}/../data/sw-instruct-no-speakers/" # SW_NO_SPEAKER_DIR data formatted with a prompt and apply_chat_template
  
  # MODEL
  MODEL_NAME_OR_DIR = "/disk/data2/s2482679/models/Meta-Llama-3-8B-Instruct" # Llama-3-8B-Instruct
  USE_SAFETENSORS = True # set to true if MODEL_NAME_OR_DIR is a local directory (safetensors are downloaded)
  PROMPTS_DIR = f"{dir}/../soft-prompts/" # directory where trained model is stored and retrieved from
  MAX_LENGTH = 4096 # How many tokens to set each conversation to for training & perplexity evaluation

  # QUANTIZATION
  quantize = False # set to true to use bnb_config. If false, sets quantization_config to None
  
  bnb_config = BitsAndBytesConfig(
    load_in_4bit=quantize,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
  )
  ATTENTION = "flash_attention_2" if quantize else None

  # TRAINING
  DO_TRAINING = True # if False, skips the training stage.
  BATCH_SIZE = 2 # Training & evaluation batch size
  EPOCHS = 3
  VIRTUAL_TOKENS = 16
  PROMPT_TEXT = "Write a phone conversation between two people discussing a topic."
  DROPOUT = 0.1
  OPTIMIZER = "paged_adamw_32bit"
  TRAIN_DIR = f"{dir}/../training_checkpoints/"
  REPORT_TO = None

  # PERPLEXITY
  EVAL_BASE_MODEL = True
  EVAL_TUNED_MODEL = True
  STRIDE = 1024

  # INFERENCE
  GENERATE_USING_BASE = False # if false, skips generation with this model
  GENERATE_USING_TUNED = True # if false, skips generation with this model
  BASE_GENERATE_DIR = f"{dir}/../data/base-llama-generate" # where conversations generated by base llama are stored
  TRAINED_GENERATE_DIR = f"{dir}/../data/trained-llama-generate" # where conversations generated by fine-tuned llama are stored
  REPETITION_PENALTY = 1.1 # recommended: 1.1
  GENERATION_TOKENS = 2048 # mean number of tokens in switchboard conversation are ~1950
  USE_SAMPLING = True # recommended: True.

  # NGRAMS
  NGRAM_DATA_DIR = f"{dir}/../data/ngrams/" # where data for ngram training & testing is stored and retrieved from
  